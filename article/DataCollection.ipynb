{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# End-to-End Machine Learning Project: Twitter Sentiment Analysis - Introduction and Data Collection (Part 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I believe that creating a portfolio project is a great way to practice and showcase our skills in data science. In this post, I want to share an example of an end-to-end machine learning project on sentiment analysis, which is a rapidly growing field in natural language processing and machine learning. We will go over the entire process, from data collection and preprocessing to model building, creating a dashboard, and finally deploying the model and dashboard as an online application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before collecting the data, we need to define the objective of our project. Our objective is to predict the public's sentiment about a brand (product, service, company or person) based on tweet data. We will use the data collection methodology described in [this paper](https://www-cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf) (Twitter Sentiment Classification using Distant Supervision, Go, Bhayani, & Huang, 2009). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Distant supervision is a method that utilizes a set of rules to automatically label a dataset. Since it does not require human intervention, it can save a lot of time and resources, especially when working with large datasets. In our case, we will use emoticons to label the sentiment of the tweet. Specifically, a tweet with a smiley face emoticon will be labeled as positive, and a tweet with a frowning face emoticon will be labeled negative. We will use a library called `snscrape` to collect the tweets; it does not require using the Twitter API, so we can retrieve a large amount of tweets without worrying about the [rate limit](https://developer.twitter.com/en/docs/twitter-api/rate-limits). In the following section we will walk through the codes and explain the logic behind them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Disclaimer: As of 11 January 2023, Twitter modified its frontend API and the code below will no longer work. I will provide an alternative as soon as I find a solution.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsbaR36eluJs"
      },
      "source": [
        "First we will import the necessary libraries, please install them first if you do not already have them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpU0DMr1bzKz",
        "outputId": "374da20f-ef14-4121-eed8-819d46c36376"
      },
      "outputs": [],
      "source": [
        "!pip install snscrape\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import datetime as dt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NfIWScXlwvE"
      },
      "source": [
        "Then we will make a function that utilizes `sntwitter.TwitterSearchScraper` to retrieve the tweets and save them in a dataframe. The function takes the following arguments:  \n",
        "* search_term: the term you want to search for on Twitter\n",
        "* start_date: the start date of the search range in the format of datetime.date object\n",
        "* end_date: the end date of the search range in the format of datetime.date object\n",
        "* num_tweets: the number of tweets you want to retrieve\n",
        "\n",
        "A `for` loop is used to iterate over and store the tweet data (username, date, and tweet content) returned by the `get_items` method of `sntwitter.TwitterSearchScraper`. We use lang:en (English language) and exclude:retweets as the search filters. The tweet data is finally returned as a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvikYVSAc_3K"
      },
      "outputs": [],
      "source": [
        "def scrape_tweet(search_term, start_date, end_date, num_tweets):\n",
        "    start_date = start_date.strftime(\"%Y-%m-%d\")\n",
        "    end_date = end_date.strftime(\"%Y-%m-%d\")\n",
        "    tweet_data = []\n",
        "    for i, tweet in enumerate(\n",
        "        sntwitter.TwitterSearchScraper(\n",
        "            \"{} since:{} until:{} lang:en exclude:retweets\".format(\n",
        "                search_term, start_date, end_date\n",
        "            )\n",
        "        ).get_items()\n",
        "    ):\n",
        "        if i >= num_tweets:\n",
        "            break\n",
        "        tweet_data.append([tweet.user.username, tweet.date, tweet.content])\n",
        "    tweet_df = pd.DataFrame(tweet_data, columns=[\"username\", \"date\", \"tweet\"])\n",
        "    return tweet_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this project, we want to retrieve tweets from 2022-01-01 to 2022-12-31. So we make another function, `daily_scrape_2022` which utilizes the `scrape_tweet` function to retrieve tweets for each day in 2022. We can specify the number of tweets we want to retrieve for each day using `num_daily`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aolFlfnhUVkD"
      },
      "outputs": [],
      "source": [
        "def daily_scrape_2022(search_term, num_daily):\n",
        "    start_date = dt.datetime(2022, 1, 1)\n",
        "    end_date = dt.datetime(2022, 1, 2)\n",
        "    delta = dt.timedelta(days=1)\n",
        "    df = pd.DataFrame()\n",
        "    for n in range(365):\n",
        "        temp_df = scrape_tweet(search_term, start_date, end_date, num_daily)\n",
        "        df = pd.concat([df, temp_df])\n",
        "        start_date += delta\n",
        "        end_date += delta\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will use the `daily_scrape_2022` function to retrieve 1000 tweets daily for each day in 2022. Tweets with negative sentiment will be searched with the term \":(\" while tweets with positive sentiment will be searched with the term \":)\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gv9gRwwOU-Xj"
      },
      "outputs": [],
      "source": [
        "ori_neg_df = daily_scrape_2022(\":(\", 1000)\n",
        "ori_pos_df = daily_scrape_2022(\":)\", 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsRu_eFemImS"
      },
      "source": [
        "The retrieved tweets do not always contain the specified search term, so we need to do some filtering. We create two functions, `filter_include` to include tweets containing a specific term and `filter_exclude` to exclude tweets containing a specific term. Note that both functions take a list of terms as the second argument, so we can filter multiple terms at once. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pH8jHuVXXr5n"
      },
      "outputs": [],
      "source": [
        "def filter_include(df, term_list):\n",
        "    temp_df = pd.DataFrame()\n",
        "    for term in term_list:\n",
        "        add_df = df[df[\"tweet\"].str.contains(term, regex=False) == True]\n",
        "        temp_df = pd.concat([temp_df, add_df]).drop_duplicates(ignore_index=True)\n",
        "    return\n",
        "\n",
        "\n",
        "def filter_exclude(df, term_list):\n",
        "    temp_df = df.copy()\n",
        "    for term in term_list:\n",
        "        temp_df = temp_df[temp_df[\"tweet\"].str.contains(term, regex=False) == False]\n",
        "    return temp_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRNtDD9ouKY8"
      },
      "source": [
        "For the negative tweets, first we will include tweets containing the term \":(\" or \":-(\", then exclude tweets containing the term \":)\", \":D\", or \":-)\". Note that `filter_exclude` is done on `neg_df`, not `ori_neg_df`. Tweets with smiley face emoticon are excluded because we do not want to label tweets containing both frowning face and smiley face emoticons as negative. After filtering, we have 358624 tweets with negative sentiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rv0fhp3CZIIZ",
        "outputId": "a13341b2-cc9f-45aa-f6e7-9f9c74d18ef5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(358624, 3)"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "neg_df = filter_include(ori_neg_df, [\":(\", \":-(\"])\n",
        "neg_df = filter_exclude(neg_df, [\":)\", \":D\", \":-)\"])\n",
        "neg_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9c-PW7Svj7k"
      },
      "source": [
        "Similar filtering is done for the positive tweets. After filtering, we have 343477 tweets with positive sentiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPsMSowWZpSw",
        "outputId": "f8e29997-1f83-4410-b759-6a7f8e8cda19"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(343477, 3)"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pos_df = filter_include(ori_pos_df, [\":)\", \":D\", \":-)\"])\n",
        "pos_df = filter_exclude(pos_df, [\":(\", \":-(\"])\n",
        "pos_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAi-FFwpghg4"
      },
      "source": [
        "Next, we will remove all the emoticons used for `filter_include` from the tweets. We do this because we want our model to classify the sentiment based on the words instead of emoticons. If we include the emoticons in the training data, the model will have poor generalization performance because real-world data may not contain emoticons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANcprsmcgxsS"
      },
      "outputs": [],
      "source": [
        "def remove_term(df, term_list):\n",
        "    temp_df = df.copy()\n",
        "    for term in term_list:\n",
        "        temp_df[\"tweet\"] = temp_df[\"tweet\"].str.replace(term, \" \", regex=False)\n",
        "    return temp_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhVRQS0LhsoS"
      },
      "outputs": [],
      "source": [
        "neg_df = remove_term(neg_df, [\":(\", \":-(\"])\n",
        "pos_df = remove_term(pos_df, [\":)\", \":D\", \":-)\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8_7ZDiMj1bq"
      },
      "source": [
        "Last we will label the sentiment of the tweets and combine them into one dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lPA50lVkID8"
      },
      "outputs": [],
      "source": [
        "neg_df[\"sentiment\"] = \"Negative\"\n",
        "pos_df[\"sentiment\"] = \"Positive\"\n",
        "df = pd.concat([neg_df, pos_df]).reset_index(drop=True)\n",
        "df.to_csv(\"/content/drive/MyDrive/dataset/labeled_tweets.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So now we have collected our training data through distant supervision. In the next post, we will walk through the steps of text preprocessing and word embedding, then use it to build a Long Short-term Memory (LSTM) model. Stay tuned!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Relevant Links"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Project Github: [github.com/tmtsmrsl/TwitterSentimentAnalyzer](https://github.com/tmtsmrsl/TwitterSentimentAnalyzer)  \n",
        "Streamlit App: [twitter-sentiment.streamlit.app/](https://twitter-sentiment.streamlit.app/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "9be1022a05af415b4028b92ecacc354b62c054b161aeb0bf6140aaf31badf13b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
